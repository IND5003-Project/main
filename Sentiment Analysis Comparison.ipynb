{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import re\n",
    "import nltk\n",
    "import html\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import csv\n",
    "import time\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with pre-labelled existing data from Twitter\n",
    "raw = pd.read_csv('./Data/twitter_corpus-master/full-corpus.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = raw[raw.Sentiment != 'irrelevant']\n",
    "test.drop(['Topic', 'TweetId', 'TweetDate'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic pre-processing done on Twitter data\n",
    "\n",
    "test.loc[:, 'TweetText'] = test.loc[:, 'TweetText'].apply(lambda x: html.unescape(x))\n",
    "test.loc[:, 'TweetText'] = test.loc[:, 'TweetText'].apply(lambda x: re.sub(r'(www\\.|https?://).*?(\\s|$)|@.*?(\\s|$)|\\$.*?(\\s|$)|\\d|\\%|\\\\|/|-|_', ' ', x))\n",
    "test.loc[:, 'TweetText'] = test.loc[:, 'TweetText'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time()\n",
    "test.loc[:50, 'TextBlob Sentiment Score'] = test.loc[:50, ['TweetText']].apply(lambda x: TextBlob(x[0], analyzer=NaiveBayesAnalyzer()).sentiment[1], axis=1)\n",
    "time_TextBlob = time.process_time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_TextBlob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_TextBlob(x):\n",
    "    if x >= 0.6:\n",
    "        return \"positive\"\n",
    "    elif x <= 0.4:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "test.loc[:, 'TextBlob Sentiment'] = test.loc[:, ['TextBlob Sentiment Score']].apply(lambda x: get_class_TextBlob(x[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:, 'TextBlob Match'] = np.where(test.loc[:, 'TextBlob Sentiment'] == test.loc[:, 'Sentiment'], 'Yes', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextBlob_accuracy = (test['TextBlob Match'][:50] == 'Yes').sum() / 50\n",
    "print(TextBlob_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time() \n",
    "test.loc[:, 'VADER Sentiment Score'] = test.loc[:, ['TweetText']].apply(lambda x: sia.polarity_scores(x[0])['compound'], axis=1)\n",
    "time_VADAR = time.process_time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_VADER(x):\n",
    "    if x >= 0.3:\n",
    "        return \"positive\"\n",
    "    elif x <= -0.3:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "test.loc[:, 'VADER Sentiment'] = test.loc[:, ['VADER Sentiment Score']].apply(lambda x: get_class_VADER(x[0]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.loc[:, 'VADER Match'] = np.where(test.loc[:, 'VADER Sentiment'] == test.loc[:, 'Sentiment'], 'Yes', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VADER_accuracy = (test['VADER Match'] == 'Yes').sum() / len(test)\n",
    "print(VADER_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "# need to connect to their server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoreNLP_scores = []\n",
    "CoreNLP_ss = 0\n",
    "length_s = 0\n",
    "\n",
    "for i in range(50):\n",
    "    text = test['TweetText'][i]\n",
    "    result = nlp.annotate(text,\n",
    "                   properties={\n",
    "                       'annotators': 'sentiment, ner, pos',\n",
    "                       'outputFormat': 'json',\n",
    "                       'timeout': 15000,\n",
    "                   })\n",
    "\n",
    "    for s in result['sentences']:\n",
    "        CoreNLP_ss += int(s['sentimentValue'])\n",
    "        length_s += 1\n",
    "        \n",
    "    if (length_s > 0):\n",
    "        score = CoreNLP_ss/length_s\n",
    "    if (score < 2):\n",
    "        score_text = 'negative'\n",
    "    elif(score == 2):\n",
    "        score_text = 'neutral'\n",
    "    else: \n",
    "        score_text = 'positive'\n",
    "        \n",
    "    CoreNLP_scores.append(score_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoreNLP_sentiments_df = pd.DataFrame({'CoreNLP Sentiment':CoreNLP_scores})\n",
    "test['CoreNLP Sentiment'] = CoreNLP_sentiments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:, 'CoreNLP Match'] = np.where(test.loc[:, 'CoreNLP Sentiment'] == test.loc[:, 'Sentiment'], 'Yes', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoreNLP_accuracy = (test['CoreNLP Match'] == 'Yes').sum() / len(CoreNLP_scores)\n",
    "print(CoreNLP_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing using SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def get_sentiword_score(message):\n",
    "        \"\"\"\n",
    "            takes a message and performs following operations:\n",
    "            1) tokenize\n",
    "            2) POS tagging\n",
    "            3) reduce text to nouns, verbs, adjectives, adverbs\n",
    "            4) lemmatize the words\n",
    "            for each selected tag, if more than one sense exists, performs word sense disambiguation\n",
    "            using lesk algorithm and finally returns positivity score, negativity score from\n",
    "            sentiwordnet lexicon\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = word_tokenize(message)\n",
    "        pos = pos_tag(tokens)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        selected_tags = list()\n",
    "        scores = list()\n",
    "\n",
    "        for i in range(len(pos)):\n",
    "            if pos[i][1].startswith('J'):\n",
    "                selected_tags.append((lemmatizer.lemmatize(pos[i][0], 'a'), 'a'))\n",
    "            elif pos[i][1].startswith('V'):\n",
    "                selected_tags.append((lemmatizer.lemmatize(pos[i][0], 'v'), 'v'))\n",
    "            elif pos[i][1].startswith('N'):\n",
    "                selected_tags.append((lemmatizer.lemmatize(pos[i][0], 'n'), 'n'))\n",
    "            elif pos[i][1].startswith('R'):\n",
    "                selected_tags.append((lemmatizer.lemmatize(pos[i][0], 'r'), 'r'))\n",
    "\n",
    "        # score list: [(sense name, pos score, neg score)]\n",
    "        for i in range(len(selected_tags)):\n",
    "            senses = list(swn.senti_synsets(selected_tags[i][0], selected_tags[i][1]))\n",
    "            if len(senses) == 1:\n",
    "                scores.append((senses[0].synset.name(), senses[0].pos_score(), senses[0].neg_score()))\n",
    "            elif len(senses) > 1:\n",
    "                sense = lesk(tokens, selected_tags[i][0], selected_tags[i][1])\n",
    "                if sense is None:\n",
    "                    # take average score of all original senses\n",
    "                    pos_score = 0\n",
    "                    neg_score = 0\n",
    "                    for i in senses:\n",
    "                        pos_score += i.pos_score()\n",
    "                        neg_score += i.neg_score()\n",
    "                    scores.append((senses[0].synset.name(), pos_score/len(senses), neg_score/len(senses)))\n",
    "                else:\n",
    "                    sense = swn.senti_synset(sense.name())\n",
    "                    scores.append((sense.synset.name(), sense.pos_score(), sense.neg_score()))\n",
    "\n",
    "        \"\"\"\n",
    "            Aggregating sentiment scores:\n",
    "                Sum up the positive and negative scores\n",
    "                Whenever a negative word is encountered, reverse the positive and negative score.\n",
    "        \"\"\"\n",
    "\n",
    "        # collected from word stat financial dictionary\n",
    "        negation_words = list(open('Lexicon/lexicon_negation_words.txt').read().split())\n",
    "\n",
    "        # final_score = 0\n",
    "        # counter = 1\n",
    "        # for score in scores:\n",
    "        #     if any(score[0].startswith(x) for x in negation_words):\n",
    "        #         counter *= -1\n",
    "        #     else:\n",
    "        #         if score[1] > score[2]:\n",
    "        #             final_score += counter*score[1]\n",
    "        #         elif score[1] < score[2]:\n",
    "        #             final_score -= counter*score[2]\n",
    "\n",
    "        counter = 1\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        for score in scores:\n",
    "            if any(score[0].startswith(x) for x in negation_words):\n",
    "                counter *= -1\n",
    "            else:\n",
    "                if counter == 1:\n",
    "                    pos_score += score[1]\n",
    "                    neg_score += score[2]\n",
    "                elif counter == -1:\n",
    "                    pos_score += score[2]\n",
    "                    neg_score += score[1]\n",
    "\n",
    "        final_score = [pos_score, neg_score]\n",
    "        return final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.process_time() \n",
    "SWN_scores = []\n",
    "for x in test['TweetText']:\n",
    "    SWN_scores.append(get_sentiword_score(x))\n",
    "time_SWN = time.process_time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = []\n",
    "for x in SWN_scores:\n",
    "    if x[0] + x[1] == 0:\n",
    "        ss.append('neutral')\n",
    "    elif (x[0] / (x[0] + x[1])) > 0.6:\n",
    "        ss.append('positive')\n",
    "    elif (x[1] / (x[0] + x[1])) > 0.6:\n",
    "        ss.append('negative')\n",
    "    else: \n",
    "        ss.append('neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_df = pd.DataFrame({'SWN Sentiment':ss})\n",
    "test['SWN Sentiment'] = ss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[:, 'SWN Match'] = np.where(test.loc[:, 'SWN Sentiment'] == test.loc[:, 'Sentiment'], 'Yes', 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWN_accuracy = (test['SWN Match'] == 'Yes').sum() / len(test)\n",
    "print(SWN_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_per_text_TextBlob = time_TextBlob / 50\n",
    "time_per_text_VADAR = time_VADAR / len(test)\n",
    "# time per text for Stanford CoreNLP is manually measured and calculated\n",
    "time_per_text_SWN = time_SWN / len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average time taken per tweet (TextBlob): \", time_per_text_TextBlob)\n",
    "print(\"Average time taken per tweet (VADER): \", time_per_text_VADAR)\n",
    "print(\"Average time taken per tweet (SWN): \", time_per_text_SWN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TextBlob accuracy: \", TextBlob_accuracy)\n",
    "print(\"VADER accuracy: \", VADER_accuracy)\n",
    "print(\"CoreNLP accuracy: \", CoreNLP_accuracy)\n",
    "print(\"SWN accuracy: \", SWN_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
