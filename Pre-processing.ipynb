{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import re\n",
    "import html\n",
    "import os.path\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = './Stocktwit Raw.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_raw = pd.read_csv(file_location, header=0, encoding = 'unicode_escape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Title_link</th>\n",
       "      <th>st_2jbUWIi</th>\n",
       "      <th>st_3SL2gug</th>\n",
       "      <th>st_1NNeqUz</th>\n",
       "      <th>st_28bQfzV</th>\n",
       "      <th>st_1NNeqUz1</th>\n",
       "      <th>st_3xRimaf</th>\n",
       "      <th>lib_XwnOHoV</th>\n",
       "      <th>likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jekoz</td>\n",
       "      <td>https://stocktwits.com/jekoz</td>\n",
       "      <td>https://s3.amazonaws.com/st-avatars/images/def...</td>\n",
       "      <td>$DIS nothing can get this over 140. Maybe $AAP...</td>\n",
       "      <td>$DIS</td>\n",
       "      <td>Sep 17th, 8:40 am</td>\n",
       "      <td>$AAPL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sleekOptions</td>\n",
       "      <td>https://stocktwits.com/sleekOptions</td>\n",
       "      <td>https://avatars.stocktwits.com/production/1442...</td>\n",
       "      <td>$AAPL [Aug-30 210 Calls] up +30.43 %  Alerted ...</td>\n",
       "      <td>$AAPL</td>\n",
       "      <td>Sep 17th, 8:28 am</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sleekOptions</td>\n",
       "      <td>https://stocktwits.com/sleekOptions</td>\n",
       "      <td>https://avatars.stocktwits.com/production/1442...</td>\n",
       "      <td>Peak profit for the last 6 expired option aler...</td>\n",
       "      <td>$AAPL</td>\n",
       "      <td>Sep 17th, 8:26 am</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sleekOptions</td>\n",
       "      <td>https://stocktwits.com/sleekOptions</td>\n",
       "      <td>https://avatars.stocktwits.com/production/1442...</td>\n",
       "      <td>$AAPL Last six months, 41 option alerts peaked...</td>\n",
       "      <td>$AAPL</td>\n",
       "      <td>Sep 17th, 8:25 am</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>klrkawa</td>\n",
       "      <td>https://stocktwits.com/klrkawa</td>\n",
       "      <td>https://avatars.stocktwits.com/production/1520...</td>\n",
       "      <td>$AAPL $ROKU $DE $HD $TGT  down. Not looking go...</td>\n",
       "      <td>$AAPL</td>\n",
       "      <td>Sep 17th, 8:11 am</td>\n",
       "      <td>$ROKU</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Title                           Title_link  \\\n",
       "0         jekoz         https://stocktwits.com/jekoz   \n",
       "1  sleekOptions  https://stocktwits.com/sleekOptions   \n",
       "2  sleekOptions  https://stocktwits.com/sleekOptions   \n",
       "3  sleekOptions  https://stocktwits.com/sleekOptions   \n",
       "4       klrkawa       https://stocktwits.com/klrkawa   \n",
       "\n",
       "                                          st_2jbUWIi  \\\n",
       "0  https://s3.amazonaws.com/st-avatars/images/def...   \n",
       "1  https://avatars.stocktwits.com/production/1442...   \n",
       "2  https://avatars.stocktwits.com/production/1442...   \n",
       "3  https://avatars.stocktwits.com/production/1442...   \n",
       "4  https://avatars.stocktwits.com/production/1520...   \n",
       "\n",
       "                                          st_3SL2gug st_1NNeqUz  \\\n",
       "0  $DIS nothing can get this over 140. Maybe $AAP...       $DIS   \n",
       "1  $AAPL [Aug-30 210 Calls] up +30.43 %  Alerted ...      $AAPL   \n",
       "2  Peak profit for the last 6 expired option aler...      $AAPL   \n",
       "3  $AAPL Last six months, 41 option alerts peaked...      $AAPL   \n",
       "4  $AAPL $ROKU $DE $HD $TGT  down. Not looking go...      $AAPL   \n",
       "\n",
       "          st_28bQfzV st_1NNeqUz1  st_3xRimaf lib_XwnOHoV  likes  \n",
       "0  Sep 17th, 8:40 am       $AAPL         NaN         NaN    NaN  \n",
       "1  Sep 17th, 8:28 am         NaN         NaN         NaN    NaN  \n",
       "2  Sep 17th, 8:26 am         NaN         NaN         NaN    NaN  \n",
       "3  Sep 17th, 8:25 am         NaN         NaN         NaN    NaN  \n",
       "4  Sep 17th, 8:11 am       $ROKU         1.0     Bearish    1.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = st_raw.iloc[:,3:6]\n",
    "st.drop(['st_1NNeqUz'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>st_3SL2gug</th>\n",
       "      <th>st_28bQfzV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$DIS nothing can get this over 140. Maybe $AAP...</td>\n",
       "      <td>Sep 17th, 8:40 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL [Aug-30 210 Calls] up +30.43 %  Alerted ...</td>\n",
       "      <td>Sep 17th, 8:28 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Peak profit for the last 6 expired option aler...</td>\n",
       "      <td>Sep 17th, 8:26 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$AAPL Last six months, 41 option alerts peaked...</td>\n",
       "      <td>Sep 17th, 8:25 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$AAPL $ROKU $DE $HD $TGT  down. Not looking go...</td>\n",
       "      <td>Sep 17th, 8:11 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$AMZN $GOOGL $AAPL $VOOG always a solid buy fo...</td>\n",
       "      <td>Sep 17th, 8:02 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$AAPL China says 'vice ministerial' officials ...</td>\n",
       "      <td>Sep 17th, 7:58 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$AAPL $NET $SNAP $OXY it's awesome</td>\n",
       "      <td>Sep 17th, 7:23 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$AAPL everyone please report @ceobrattwatking....</td>\n",
       "      <td>Sep 17th, 7:19 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$AAPL ??</td>\n",
       "      <td>Sep 17th, 7:10 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$AAPL Tomorrow? Actually... no clue lol ??</td>\n",
       "      <td>Sep 17th, 7:02 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Video Analysis - $AAPL    \\nWatch Here: youtub...</td>\n",
       "      <td>Sep 17th, 6:14 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$NFLX $TSLA $AAPL $AMZN Technical Analysis Cha...</td>\n",
       "      <td>Sep 17th, 6:04 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$ROKU $AAPL $NFLX As streaming TV becomes more...</td>\n",
       "      <td>Sep 17th, 5:41 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>$AAPL Run for the Hills??????????????War is co...</td>\n",
       "      <td>Sep 17th, 5:27 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Tuesdays Watch Part 1: $AAPL $AMD $AMZN $BA $...</td>\n",
       "      <td>Sep 17th, 5:27 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>$AAPL going up IMO</td>\n",
       "      <td>Sep 17th, 5:26 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>$AAPL why didnt apple advertise this benefit....</td>\n",
       "      <td>Sep 17th, 5:24 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>?? $AAPL Could Apple See a Rebound in iPhone S...</td>\n",
       "      <td>Sep 17th, 5:24 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>$AAPL Playing Where Cards Fall in Apple Arca...</td>\n",
       "      <td>Sep 17th, 5:24 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>$AAPL ITS APPLE WEEK\\nMon: Arcade\\nTue: iPhon...</td>\n",
       "      <td>Sep 17th, 5:22 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>$AMTD $SPY $AAPL Any of you guys using Thinkor...</td>\n",
       "      <td>Sep 17th, 5:18 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>$AAPL i am gonna keep adding to short position...</td>\n",
       "      <td>Sep 17th, 5:11 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>$AAPL trade war goes on, and manufacturer are ...</td>\n",
       "      <td>Sep 17th, 5:03 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>$SPY $AAPL trade meeting in US</td>\n",
       "      <td>Sep 17th, 5:02 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>$AAPL I dont think I want to go long at this l...</td>\n",
       "      <td>Sep 17th, 4:59 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>$AAPL Any new investors? Were good. Parabolic...</td>\n",
       "      <td>Sep 17th, 4:54 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Swing trading watchlist $AAPL $TRIP updated ni...</td>\n",
       "      <td>Sep 17th, 4:52 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>$AAPL ARCADE! This is Huge! Kids will fall in ...</td>\n",
       "      <td>Sep 17th, 4:48 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>$AAPL Although the basic form of the store wil...</td>\n",
       "      <td>Sep 17th, 4:44 am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           st_3SL2gug         st_28bQfzV\n",
       "0   $DIS nothing can get this over 140. Maybe $AAP...  Sep 17th, 8:40 am\n",
       "1   $AAPL [Aug-30 210 Calls] up +30.43 %  Alerted ...  Sep 17th, 8:28 am\n",
       "2   Peak profit for the last 6 expired option aler...  Sep 17th, 8:26 am\n",
       "3   $AAPL Last six months, 41 option alerts peaked...  Sep 17th, 8:25 am\n",
       "4   $AAPL $ROKU $DE $HD $TGT  down. Not looking go...  Sep 17th, 8:11 am\n",
       "5   $AMZN $GOOGL $AAPL $VOOG always a solid buy fo...  Sep 17th, 8:02 am\n",
       "6   $AAPL China says 'vice ministerial' officials ...  Sep 17th, 7:58 am\n",
       "7                  $AAPL $NET $SNAP $OXY it's awesome  Sep 17th, 7:23 am\n",
       "8   $AAPL everyone please report @ceobrattwatking....  Sep 17th, 7:19 am\n",
       "9                                            $AAPL ??  Sep 17th, 7:10 am\n",
       "10         $AAPL Tomorrow? Actually... no clue lol ??  Sep 17th, 7:02 am\n",
       "11  Video Analysis - $AAPL    \\nWatch Here: youtub...  Sep 17th, 6:14 am\n",
       "12  $NFLX $TSLA $AAPL $AMZN Technical Analysis Cha...  Sep 17th, 6:04 am\n",
       "13  $ROKU $AAPL $NFLX As streaming TV becomes more...  Sep 17th, 5:41 am\n",
       "14  $AAPL Run for the Hills??????????????War is co...  Sep 17th, 5:27 am\n",
       "15  Tuesdays Watch Part 1: $AAPL $AMD $AMZN $BA $...  Sep 17th, 5:27 am\n",
       "16                                 $AAPL going up IMO  Sep 17th, 5:26 am\n",
       "17  $AAPL why didnt apple advertise this benefit....  Sep 17th, 5:24 am\n",
       "18  ?? $AAPL Could Apple See a Rebound in iPhone S...  Sep 17th, 5:24 am\n",
       "19  $AAPL Playing Where Cards Fall in Apple Arca...  Sep 17th, 5:24 am\n",
       "20  $AAPL ITS APPLE WEEK\\nMon: Arcade\\nTue: iPhon...  Sep 17th, 5:22 am\n",
       "21  $AMTD $SPY $AAPL Any of you guys using Thinkor...  Sep 17th, 5:18 am\n",
       "22  $AAPL i am gonna keep adding to short position...  Sep 17th, 5:11 am\n",
       "23  $AAPL trade war goes on, and manufacturer are ...  Sep 17th, 5:03 am\n",
       "24                     $SPY $AAPL trade meeting in US  Sep 17th, 5:02 am\n",
       "25  $AAPL I dont think I want to go long at this l...  Sep 17th, 4:59 am\n",
       "26  $AAPL Any new investors? Were good. Parabolic...  Sep 17th, 4:54 am\n",
       "27  Swing trading watchlist $AAPL $TRIP updated ni...  Sep 17th, 4:52 am\n",
       "28  $AAPL ARCADE! This is Huge! Kids will fall in ...  Sep 17th, 4:48 am\n",
       "29  $AAPL Although the basic form of the store wil...  Sep 17th, 4:44 am"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.columns=['message', 'datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$DIS nothing can get this over 140. Maybe $AAP...</td>\n",
       "      <td>Sep 17th, 8:40 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL [Aug-30 210 Calls] up +30.43 %  Alerted ...</td>\n",
       "      <td>Sep 17th, 8:28 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Peak profit for the last 6 expired option aler...</td>\n",
       "      <td>Sep 17th, 8:26 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$AAPL Last six months, 41 option alerts peaked...</td>\n",
       "      <td>Sep 17th, 8:25 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$AAPL $ROKU $DE $HD $TGT  down. Not looking go...</td>\n",
       "      <td>Sep 17th, 8:11 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$AMZN $GOOGL $AAPL $VOOG always a solid buy fo...</td>\n",
       "      <td>Sep 17th, 8:02 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$AAPL China says 'vice ministerial' officials ...</td>\n",
       "      <td>Sep 17th, 7:58 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$AAPL $NET $SNAP $OXY it's awesome</td>\n",
       "      <td>Sep 17th, 7:23 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$AAPL everyone please report @ceobrattwatking....</td>\n",
       "      <td>Sep 17th, 7:19 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$AAPL ??</td>\n",
       "      <td>Sep 17th, 7:10 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$AAPL Tomorrow? Actually... no clue lol ??</td>\n",
       "      <td>Sep 17th, 7:02 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Video Analysis - $AAPL    \\nWatch Here: youtub...</td>\n",
       "      <td>Sep 17th, 6:14 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$NFLX $TSLA $AAPL $AMZN Technical Analysis Cha...</td>\n",
       "      <td>Sep 17th, 6:04 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$ROKU $AAPL $NFLX As streaming TV becomes more...</td>\n",
       "      <td>Sep 17th, 5:41 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>$AAPL Run for the Hills??????????????War is co...</td>\n",
       "      <td>Sep 17th, 5:27 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Tuesdays Watch Part 1: $AAPL $AMD $AMZN $BA $...</td>\n",
       "      <td>Sep 17th, 5:27 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>$AAPL going up IMO</td>\n",
       "      <td>Sep 17th, 5:26 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>$AAPL why didnt apple advertise this benefit....</td>\n",
       "      <td>Sep 17th, 5:24 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>?? $AAPL Could Apple See a Rebound in iPhone S...</td>\n",
       "      <td>Sep 17th, 5:24 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>$AAPL Playing Where Cards Fall in Apple Arca...</td>\n",
       "      <td>Sep 17th, 5:24 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>$AAPL ITS APPLE WEEK\\nMon: Arcade\\nTue: iPhon...</td>\n",
       "      <td>Sep 17th, 5:22 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>$AMTD $SPY $AAPL Any of you guys using Thinkor...</td>\n",
       "      <td>Sep 17th, 5:18 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>$AAPL i am gonna keep adding to short position...</td>\n",
       "      <td>Sep 17th, 5:11 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>$AAPL trade war goes on, and manufacturer are ...</td>\n",
       "      <td>Sep 17th, 5:03 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>$SPY $AAPL trade meeting in US</td>\n",
       "      <td>Sep 17th, 5:02 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>$AAPL I dont think I want to go long at this l...</td>\n",
       "      <td>Sep 17th, 4:59 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>$AAPL Any new investors? Were good. Parabolic...</td>\n",
       "      <td>Sep 17th, 4:54 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Swing trading watchlist $AAPL $TRIP updated ni...</td>\n",
       "      <td>Sep 17th, 4:52 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>$AAPL ARCADE! This is Huge! Kids will fall in ...</td>\n",
       "      <td>Sep 17th, 4:48 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>$AAPL Although the basic form of the store wil...</td>\n",
       "      <td>Sep 17th, 4:44 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2803</th>\n",
       "      <td>$AAPL check the chart and you will understand ...</td>\n",
       "      <td>Sep 12th, 8:10 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2804</th>\n",
       "      <td>$AAPL weak again as it tries at 226.\\n\\nYester...</td>\n",
       "      <td>Sep 12th, 8:09 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>$AAPL I am ready for ??????</td>\n",
       "      <td>Sep 12th, 8:08 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2806</th>\n",
       "      <td>$AAPL 226.87 gap to fill still</td>\n",
       "      <td>Sep 12th, 8:07 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>$AAPL had a great lunch and now Im ready for ...</td>\n",
       "      <td>Sep 12th, 8:06 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2808</th>\n",
       "      <td>$AAPL WS wants more time to push.</td>\n",
       "      <td>Sep 12th, 8:05 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>$AAPL To the moon.</td>\n",
       "      <td>Sep 12th, 8:04 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2810</th>\n",
       "      <td>$AAPL $NVDA $NFLX more good trade news coming ...</td>\n",
       "      <td>Sep 12th, 8:04 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>#OptionBlock is NOW LIVE with @Fidelity @optio...</td>\n",
       "      <td>Sep 12th, 8:03 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2812</th>\n",
       "      <td>$AAPL Doing good WS! $$$$$$$$$$$$$$$$$$$$$$$$$...</td>\n",
       "      <td>Sep 12th, 8:02 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2813</th>\n",
       "      <td>2/ Best managed #ConsumerElectronics companies...</td>\n",
       "      <td>Sep 12th, 8:00 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2814</th>\n",
       "      <td>Congrats $AAPL bulls!! I've sold with a good g...</td>\n",
       "      <td>Sep 12th, 7:59 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2815</th>\n",
       "      <td>$AAPL next resistance 227</td>\n",
       "      <td>Sep 12th, 7:59 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2816</th>\n",
       "      <td>$AAPL you'll see everyone with an iPhone 8 or ...</td>\n",
       "      <td>Sep 12th, 7:58 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2817</th>\n",
       "      <td>$AAPL every time there is slight drop yall be...</td>\n",
       "      <td>Sep 12th, 7:57 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2818</th>\n",
       "      <td>$AAPL IMO... option flow is closing out big 9/...</td>\n",
       "      <td>Sep 12th, 7:55 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2819</th>\n",
       "      <td>$SPY $QQQ $IWM $AAPL  Bull market rolls on....</td>\n",
       "      <td>Sep 12th, 7:54 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2820</th>\n",
       "      <td>I rather hold more shares of $AAPL than $FB or...</td>\n",
       "      <td>Sep 12th, 7:52 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2821</th>\n",
       "      <td>$SPY $QQQ $AAPL \\n\\nLast time I post this.  QQ...</td>\n",
       "      <td>Sep 12th, 7:52 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>$AAPL you think pre orders will effect stock p...</td>\n",
       "      <td>Sep 12th, 7:49 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>$AAPL The drop is coming at us! Be ready my fr...</td>\n",
       "      <td>Sep 12th, 7:49 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2824</th>\n",
       "      <td>$AAPL AAPL Sep 27 2019 220 Put (Weekly) . Aver...</td>\n",
       "      <td>Sep 12th, 7:48 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>$AAPL afternoon rip soon?</td>\n",
       "      <td>Sep 12th, 7:47 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2826</th>\n",
       "      <td>$AAPL Ya just look at the chart and you can se...</td>\n",
       "      <td>Sep 12th, 7:47 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2827</th>\n",
       "      <td>$SPY $AAPL\\n\\nApple 5min chart retesting break...</td>\n",
       "      <td>Sep 12th, 7:46 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>$AAPL 240+ in september, and then runs in october</td>\n",
       "      <td>Sep 12th, 7:44 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>$AAPL still a lot of upside.</td>\n",
       "      <td>Sep 12th, 7:44 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>$AAPL 202.15</td>\n",
       "      <td>Sep 12th, 7:43 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>pssst $ES_F $SPY bulls.   $AAPL primed. get it...</td>\n",
       "      <td>Sep 12th, 7:43 pm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>$AAPL load now before its too late !</td>\n",
       "      <td>Sep 12th, 7:43 pm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2833 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                message           datetime\n",
       "0     $DIS nothing can get this over 140. Maybe $AAP...  Sep 17th, 8:40 am\n",
       "1     $AAPL [Aug-30 210 Calls] up +30.43 %  Alerted ...  Sep 17th, 8:28 am\n",
       "2     Peak profit for the last 6 expired option aler...  Sep 17th, 8:26 am\n",
       "3     $AAPL Last six months, 41 option alerts peaked...  Sep 17th, 8:25 am\n",
       "4     $AAPL $ROKU $DE $HD $TGT  down. Not looking go...  Sep 17th, 8:11 am\n",
       "...                                                 ...                ...\n",
       "2828  $AAPL 240+ in september, and then runs in october  Sep 12th, 7:44 pm\n",
       "2829                       $AAPL still a lot of upside.  Sep 12th, 7:44 pm\n",
       "2830                                       $AAPL 202.15  Sep 12th, 7:43 pm\n",
       "2831  pssst $ES_F $SPY bulls.   $AAPL primed. get it...  Sep 12th, 7:43 pm\n",
       "2832               $AAPL load now before its too late !  Sep 12th, 7:43 pm\n",
       "\n",
       "[2833 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(a):\n",
    "    print(st[\"message\"][a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "st[\"message\"][x] = st['message'].apply(lambda x: ' '.join(re.findall('[A-Z][^A-Z]*', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    preprocess the data in file location and saves it as a csv file (appending '_preprocessed' before '.csv). The preprocessing us in following ways:\n",
    "    1) extract message and datetime columns.\n",
    "    2) sort according to datetime in descending order (newest first)\n",
    "    3) remove links, @ and $ references, extra whitespaces, extra '.', digits, slashes, hyphens\n",
    "    4) decode html entities\n",
    "    5) convert everything to lower case\n",
    "\"\"\"\n",
    "st.sort_values(by='datetime', ascending=False)\n",
    "\n",
    "st['message'] = st['message'].apply(lambda x: html.unescape(x))\n",
    "st['message'] = st['message'].apply(lambda x: re.sub(r'(www\\.|https?://).*?(\\s|$)|@.*?(\\s|$)|\\$.*?(\\s|$)|\\d|\\%|\\\\|/|-|_', ' ', x))\n",
    "st['message'] = st['message'].apply(lambda x: re.sub(r'\\.+', '. ', x))\n",
    "st['message'] = st['message'].apply(lambda x: re.sub(r'\\,+', ', ', x))\n",
    "st['message'] = st['message'].apply(lambda x: re.sub(r'\\?+', '? ', x))\n",
    "st['message'] = st['message'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "st['message'] = st['message'].apply(lambda x: x.lower())\n",
    "\n",
    "st.to_csv(file_location[:-4]+'_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.sort_values(by='datetime', ascending=False)\n",
    "\n",
    "st['message'] = st['message'].apply(lambda x: html.unescape(x))\n",
    "st['message'] = st['message'].apply(lambda x: re.sub(r'(www\\.|https?://).*?(\\s|$)|@.*?(\\s|$)|\\$.*?(\\s|$)|\\d|\\%|\\\\|/|-|_', ' ', x))\n",
    "st['message'] = st['message'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "\n",
    "st.to_csv(file_location[:-4]+'_preprocessed2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "st2 = pd.read_csv('./Stocktwit Raw_preprocessed2.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nothing can get this over . Maybe buyout will...</td>\n",
       "      <td>Sep 17th, 8:40 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Aug Calls] up + . Alerted at on Aug : AM Pea...</td>\n",
       "      <td>Sep 17th, 8:28 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Peak profit for the last expired option alerts...</td>\n",
       "      <td>Sep 17th, 8:26 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Last six months, option alerts peaked above p...</td>\n",
       "      <td>Sep 17th, 8:25 am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>down. Not looking good for Tuesday. Fed will ...</td>\n",
       "      <td>Sep 17th, 8:11 am</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message           datetime\n",
       "0   nothing can get this over . Maybe buyout will...  Sep 17th, 8:40 am\n",
       "1   [Aug Calls] up + . Alerted at on Aug : AM Pea...  Sep 17th, 8:28 am\n",
       "2  Peak profit for the last expired option alerts...  Sep 17th, 8:26 am\n",
       "3   Last six months, option alerts peaked above p...  Sep 17th, 8:25 am\n",
       "4   down. Not looking good for Tuesday. Fed will ...  Sep 17th, 8:11 am"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nothing can get this over . Maybe buyout will do it. Like tops i guess.\n",
      " [Aug Calls] up + . Alerted at on Aug : AM Peak after alert on \n",
      "Peak profit for the last expired option alerts for . | . | . | . | . | . |\n",
      " Last six months, option alerts peaked above percent after they were posted\n",
      " down. Not looking good for Tuesday. Fed will make it worse. Market Watch: Asian markets slip as tensions remain high after attack on Saudi oil facilities\n",
      " always a solid buy for portfolio. Also, now over followers & gaining, you guys are awesome! ??\n",
      " China says 'vice ministerial' officials will be in Washington for trade talks cnbc.com china s...\n",
      " it's awesome\n",
      " everyone please report All he does is spam every company with technical analysis Videos that arent even analyzing anything just a stupid crappy way to get you views for his pathetic YouTube channel\n",
      " ??\n"
     ]
    }
   ],
   "source": [
    "for x in range(10):\n",
    "    print(st[\"message\"][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    extract keywords from labelled stocktwits data for improved accuracy in scoring for each labelled message do\n",
    "    1) tokenize the message\n",
    "    2) perform POS tagging\n",
    "    3) if a sense is present in wordnet then, lemmatize the word and remove stop words else ignore the word\n",
    "    remove intersections from the two lists before saving\n",
    "\"\"\"\n",
    "\n",
    "        st_pre = LoadData.get_labelled_data()\n",
    "        bullish_keywords = set()\n",
    "        bearish_keywords = set()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for index, row in dataFrame.iterrows():\n",
    "            tokens = word_tokenize(row['message'])\n",
    "            pos = pos_tag(tokens)\n",
    "            selected_tags = set()\n",
    "\n",
    "            for i in range(len(pos)):\n",
    "                if len(wordnet.synsets(pos[i][0])):\n",
    "                    if pos[i][1].startswith('J'):\n",
    "                        selected_tags.add(lemmatizer.lemmatize(pos[i][0], 'a'))\n",
    "                    elif pos[i][1].startswith('V'):\n",
    "                        selected_tags.add(lemmatizer.lemmatize(pos[i][0], 'v'))\n",
    "                    elif pos[i][1].startswith('N'):\n",
    "                        selected_tags.add(lemmatizer.lemmatize(pos[i][0], 'n'))\n",
    "                    elif pos[i][1].startswith('R'):\n",
    "                        selected_tags.add(lemmatizer.lemmatize(pos[i][0], 'r'))\n",
    "            selected_tags -= stop_words\n",
    "\n",
    "            if row['sentiment'] == 'Bullish':\n",
    "                bullish_keywords = bullish_keywords.union(selected_tags)\n",
    "            elif row['sentiment'] == 'Bearish':\n",
    "                bearish_keywords = bearish_keywords.union(selected_tags)\n",
    "\n",
    "        updated_bullish_keywords = bullish_keywords - bearish_keywords\n",
    "        updated_bearish_keywords = bearish_keywords - bullish_keywords\n",
    "        with open('data-extractor/lexicon_bullish_words.txt', 'a') as file:\n",
    "            for word in updated_bullish_keywords:\n",
    "                file.write(word+\"\\n\")\n",
    "        with open('data-extractor/lexicon_bearish_words.txt', 'a') as file:\n",
    "            for word in updated_bearish_keywords:\n",
    "                file.write(word+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    handle preprocessing and loading of data.\n",
    "\"\"\"\n",
    "\n",
    "import html\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "class LoadData:\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess_stocktwits_data(cls, file_location, columns=['datetime', 'message']):\n",
    "        \"\"\"\n",
    "            preprocess the data in file location and saves it as a csv file (appending\n",
    "            '_preprocessed' before '.csv). The preprocessing us in following ways:\n",
    "            1) extract message and datetime columns.\n",
    "            2) sort according to datetime in descending order (newest first)\n",
    "            3) remove links, @ and $ references, extra whitespaces, extra '.', digits, slashes,\n",
    "            hyphons\n",
    "            4) decode html entities\n",
    "            5) convert everything to lower case\n",
    "        \"\"\"\n",
    "\n",
    "        if 'datetime' in columns:\n",
    "            dataFrame = pd.read_csv(file_location, usecols=columns, parse_dates=['datetime'], infer_datetime_format=True)\n",
    "            dataFrame.sort_values(by='datetime', ascending=False)\n",
    "        else:\n",
    "            dataFrame = pd.read_csv(file_location, usecols=columns)\n",
    "\n",
    "        dataFrame['message'] = dataFrame['message'].apply(lambda x: html.unescape(x))\n",
    "        dataFrame['message'] = dataFrame['message'].apply(lambda x: re.sub(r'(www\\.|https?://).*?(\\s|$)|@.*?(\\s|$)|\\$.*?(\\s|$)|\\d|\\%|\\\\|/|-|_', ' ', x))\n",
    "        dataFrame['message'] = dataFrame['message'].apply(lambda x: re.sub(r'\\.+', '. ', x))\n",
    "        dataFrame['message'] = dataFrame['message'].apply(lambda x: re.sub(r'\\,+', ', ', x))\n",
    "        dataFrame['message'] = dataFrame['message'].apply(lambda x: re.sub(r'\\?+', '? ', x))\n",
    "        dataFrame['message'] = dataFrame['message'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "        dataFrame['message'] = dataFrame['message'].apply(lambda x: x.lower())\n",
    "\n",
    "        dataFrame.to_csv(file_location[:-4]+'_preprocessed.csv', index=False)\n",
    "\n",
    "    @classmethod\n",
    "    def labelled_data_lexicon_analysis(cls):\n",
    "        \"\"\"\n",
    "            extract keywords from labelled stocktwits data for improved accuracy in scoring\n",
    "            for each labelled message do\n",
    "            1) tokenize the message\n",
    "            2) perform POS tagging\n",
    "            3) if a sense is present in wordnet then, lemmatize the word and remove stop words else ignore the word\n",
    "            remove intersections from the two lists before saving\n",
    "        \"\"\"\n",
    "\n",
    "        dataFrame = LoadData.get_labelled_data()\n",
    "        bullish_keywords = set()\n",
    "        bearish_keywords = set()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for index, row in dataFrame.iterrows():\n",
    "            tokens = word_tokenize(row['message'])\n",
    "            pos = pos_tag(tokens)\n",
    "            selected_tags = set()\n",
    "\n",
    "            for i in range(len(pos)):\n",
    "                if len(wordnet.synsets(pos[i][0])):\n",
    "                    if pos[i][1].startswith('J'):\n",
    "                        selected_tags.add(lemmatizer.lemmatize(pos[i][0], 'a'))\n",
    "                    elif pos[i][1].startswith('V'):\n",
    "                        selected_tags.add(lemmatizer.lemmatize(pos[i][0], 'v'))\n",
    "                    elif pos[i][1].startswith('N'):\n",
    "                        selected_tags.add(lemmatizer.lemmatize(pos[i][0], 'n'))\n",
    "                    elif pos[i][1].startswith('R'):\n",
    "                        selected_tags.add(lemmatizer.lemmatize(pos[i][0], 'r'))\n",
    "            selected_tags -= stop_words\n",
    "\n",
    "            if row['sentiment'] == 'Bullish':\n",
    "                bullish_keywords = bullish_keywords.union(selected_tags)\n",
    "            elif row['sentiment'] == 'Bearish':\n",
    "                bearish_keywords = bearish_keywords.union(selected_tags)\n",
    "\n",
    "        updated_bullish_keywords = bullish_keywords - bearish_keywords\n",
    "        updated_bearish_keywords = bearish_keywords - bullish_keywords\n",
    "        with open('data-extractor/lexicon_bullish_words.txt', 'a') as file:\n",
    "            for word in updated_bullish_keywords:\n",
    "                file.write(word+\"\\n\")\n",
    "        with open('data-extractor/lexicon_bearish_words.txt', 'a') as file:\n",
    "            for word in updated_bearish_keywords:\n",
    "                file.write(word+\"\\n\")\n",
    "\n",
    "    @classmethod\n",
    "    def get_stocktwits_data(cls, symbol):\n",
    "        \"\"\"\n",
    "            get_data loads the preprocessed data of 'symbol' from data-extractor\n",
    "            and returns a pandas dataframe with columns [message(object), datetime(datetime64[ns])].\n",
    "        \"\"\"\n",
    "\n",
    "        file_location = 'data-extractor/stocktwits_'+symbol+'_preprocessed.csv'\n",
    "        if os.path.isfile(file_location) is False:\n",
    "            LoadData.preprocess_stocktwits_data('data-extractor/stocktwits_'+symbol+'.csv')\n",
    "\n",
    "        dataFrame = pd.read_csv(file_location)\n",
    "        return dataFrame\n",
    "\n",
    "    @classmethod\n",
    "    def get_price_data(cls, symbol):\n",
    "        \"\"\"\n",
    "            loads the price data of 'symbol' from data-extractor\n",
    "            and returns a pandas dataframe with columns [Date(datetime64[ns]), Opening Price(float64), Closing Price(float64), Volume(float64)].\n",
    "        \"\"\"\n",
    "\n",
    "        file_location = 'data-extractor/stock_prices_'+symbol+'.csv'\n",
    "        dataFrame = pd.read_csv(file_location, usecols=['Date', 'Opening Price', 'Closing Price', 'Volume'], parse_dates=['Date'], infer_datetime_format=True)\n",
    "        return dataFrame\n",
    "\n",
    "    @classmethod\n",
    "    def get_labelled_data(cls, type='complete'):\n",
    "        \"\"\"\n",
    "            get_labelled_data loads the preprocessed labelled data of stocktwits from data-extractor\n",
    "            and returns a pandas dataframe with columns [sentiment(object), message(object)].\n",
    "        \"\"\"\n",
    "\n",
    "        if type == 'complete':\n",
    "            file_location = 'data-extractor/labelled_data_complete_preprocessed.csv'\n",
    "            if os.path.isfile(file_location) is False:\n",
    "                LoadData.preprocess_stocktwits_data('data-extractor/labelled_data_complete.csv', columns=['sentiment', 'message'])\n",
    "        elif type == 'training':\n",
    "            file_location = 'data-extractor/labelled_data_training_preprocessed.csv'\n",
    "            if os.path.isfile(file_location) is False:\n",
    "                LoadData.get_training_data()\n",
    "        elif type == 'test':\n",
    "            file_location = 'data-extractor/labelled_data_test_preprocessed.csv'\n",
    "            if os.path.isfile(file_location) is False:\n",
    "                LoadData.preprocess_stocktwits_data('data-extractor/labelled_data_test.csv', columns=['sentiment', 'message'])\n",
    "\n",
    "        dataFrame = pd.read_csv(file_location)\n",
    "        return dataFrame\n",
    "\n",
    "    @classmethod\n",
    "    def get_custom_lexicon(cls):\n",
    "        \"\"\"\n",
    "            get custom lexicon of bearish and bullish words respectively\n",
    "        \"\"\"\n",
    "\n",
    "        file_location1 = 'data-extractor/lexicon_bearish_words.txt'\n",
    "        file_location2 = 'data-extractor/lexicon_bullish_words.txt'\n",
    "        if os.path.isfile(file_location1) is False or os.path.isfile(file_location2) is False:\n",
    "            LoadData.labelled_data_lexicon_analysis()\n",
    "\n",
    "        dataFrameBearish = pd.read_csv(file_location1, header=None, names=['word'])\n",
    "        dataFrameBullish = pd.read_csv(file_location2, header=None, names=['word'])\n",
    "        return dataFrameBearish, dataFrameBullish\n",
    "\n",
    "    @classmethod\n",
    "    def get_training_data(cls):\n",
    "        \"\"\"\n",
    "            get labelled training data with equal bearish and bullish messages\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.remove('data-extractor/labelled_data_training.csv')\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        dataFrame = LoadData.get_labelled_data(type='complete')\n",
    "        dataFrameBearish = dataFrame[dataFrame['sentiment']=='Bearish']\n",
    "        dataFrameBullish = dataFrame[dataFrame['sentiment']=='Bullish']\n",
    "        dataFrameBearishTraining = dataFrameBearish\n",
    "        dataFrameBullishTraining = dataFrameBullish[:len(dataFrameBearish)]\n",
    "\n",
    "        dataFrameTraining = dataFrameBearishTraining.append(dataFrameBullishTraining, ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "        dataFrameTraining.to_csv('data-extractor/labelled_data_training_preprocessed.csv', index=False)\n",
    "\n",
    "    @classmethod\n",
    "    def combine_price_and_sentiment(cls, sentimentFrame, priceFrame):\n",
    "        from datetime import timedelta\n",
    "\n",
    "        \"\"\"\n",
    "            receive sentimentFrame as (date, sentiment, message) indexed by date and sentiment\n",
    "            and priceFrame as (Date, Opening Price, Closing Price, Volume) and return a combined\n",
    "            frame as (sentiment_calculated_bullish, sentiment_calculated_bearish,\n",
    "            sentiment_actual_previous, tweet_volume_change, cash_volume, label)\n",
    "        \"\"\"\n",
    "\n",
    "        dataFrame = pd.DataFrame()\n",
    "        for date, df in sentimentFrame.groupby(level=0, sort=False):\n",
    "\n",
    "            price_current = priceFrame[priceFrame['Date'] == date]\n",
    "            if price_current.empty or date-timedelta(days=1) not in sentimentFrame.index:\n",
    "                continue\n",
    "            tweet_minus1 = sentimentFrame.loc[date-timedelta(days=1)]\n",
    "            days = 1\n",
    "            price_plus1 = priceFrame[priceFrame['Date'] == date+timedelta(days=days)]\n",
    "            while price_plus1.empty:\n",
    "                days += 1\n",
    "                price_plus1 = priceFrame[priceFrame['Date'] == date+timedelta(days=days)]\n",
    "            days = 1\n",
    "            price_minus1 = priceFrame[priceFrame['Date'] == date-timedelta(days=days)]\n",
    "            while price_minus1.empty:\n",
    "                days += 1\n",
    "                price_minus1 = priceFrame[priceFrame['Date'] == date-timedelta(days=days)]\n",
    "\n",
    "            new_row = {}\n",
    "            new_row['date'] = date\n",
    "            new_row['sentiment_calculated_bullish'] = df.loc[(date, 'Bullish')]['message']\n",
    "            new_row['sentiment_calculated_bearish'] = df.loc[(date, 'Bearish')]['message']\n",
    "            new_row['sentiment_actual_previous'] = 1 if ((price_minus1.iloc[0]['Closing Price'] - price_minus1.iloc[0]['Opening Price']) >= 0) else -1\n",
    "            new_row['tweet_volume_change'] = df['message'].sum() - tweet_minus1['message'].sum()\n",
    "            new_row['cash_volume'] = price_current['Volume'].iloc[0]\n",
    "            new_row['label'] = 1 if ((price_plus1.iloc[0]['Closing Price'] - price_current.iloc[0]['Closing Price']) >= 0) else -1\n",
    "            print(new_row)\n",
    "            dataFrame = dataFrame.append(new_row, ignore_index=True)\n",
    "\n",
    "        return dataFrame\n",
    "\n",
    "    @classmethod\n",
    "    def aggregate_stock_price_data(cls):\n",
    "        \"\"\"\n",
    "            compile stocktwits data for stock prediction analysis in the following form\n",
    "            (date, sentiment_calculated_bullish, sentiment_calculated_bearish, sentiment_actual_previous, tweet_volume_change, cash_volume, label)\n",
    "            we have choice to take previous n days sentiment_calculated and using label of next nth day\n",
    "            returns dataframes for AAPL, AMZN, GOOGL respectively\n",
    "        \"\"\"\n",
    "\n",
    "        if not (os.path.isfile('data-extractor/stocktwits_AAPL_sharedata.csv') and os.path.isfile('data-extractor/stocktwits_AMZN_sharedata.csv') and os.path.isfile('data-extractor/stocktwits_GOOGL_sharedata.csv')):\n",
    "\n",
    "            from sklearn.externals import joblib\n",
    "            file_location = 'naive_bayes_classifier.pkl'\n",
    "\n",
    "            priceAAPL = LoadData.get_price_data('AAPL')\n",
    "            priceAMZN = LoadData.get_price_data('AMZN')\n",
    "            priceGOOGL = LoadData.get_price_data('GOOGL')\n",
    "\n",
    "            sentimented_file = 'data-extractor/stocktwits_AAPL_withsentiment.csv'\n",
    "            if os.path.isfile(sentimented_file) is False:\n",
    "                tweet_classifier = joblib.load(file_location)\n",
    "                dataAAPL = LoadData.get_stocktwits_data('AAPL')\n",
    "                dataAAPL['sentiment'] = dataAAPL['message'].apply(lambda x: tweet_classifier.predict([x])[0])\n",
    "                dataAAPL['datetime'] = dataAAPL['datetime'].apply(lambda x: x.date())\n",
    "                dataAAPL.rename(columns={'datetime':'date'}, inplace=True)\n",
    "                dataAAPL.to_csv('data-extractor/stocktwits_AAPL_withsentiment.csv', index=False)\n",
    "            sentimented_file = 'data-extractor/stocktwits_AMZN_withsentiment.csv'\n",
    "            if os.path.isfile(sentimented_file) is False:\n",
    "                tweet_classifier = joblib.load(file_location)\n",
    "                dataAMZN = LoadData.get_stocktwits_data('AMZN')\n",
    "                dataAMZN['sentiment'] = dataAMZN['message'].apply(lambda x: tweet_classifier.predict([x])[0])\n",
    "                dataAMZN['datetime'] = dataAMZN['datetime'].apply(lambda x: x.date())\n",
    "                dataAMZN.rename(columns={'datetime':'date'}, inplace=True)\n",
    "                dataAMZN.to_csv('data-extractor/stocktwits_AMZN_withsentiment.csv', index=False)\n",
    "            sentimented_file = 'data-extractor/stocktwits_GOOGL_withsentiment.csv'\n",
    "            if os.path.isfile(sentimented_file) is False:\n",
    "                tweet_classifier = joblib.load(file_location)\n",
    "                dataGOOGL = LoadData.get_stocktwits_data('GOOGL')\n",
    "                dataGOOGL['sentiment'] = dataGOOGL['message'].apply(lambda x: tweet_classifier.predict([x])[0])\n",
    "                dataGOOGL['datetime'] = dataGOOGL['datetime'].apply(lambda x: x.date())\n",
    "                dataGOOGL.rename(columns={'datetime':'date'}, inplace=True)\n",
    "                dataGOOGL.to_csv('data-extractor/stocktwits_GOOGL_withsentiment.csv', index=False)\n",
    "\n",
    "            dataAAPL = pd.read_csv('data-extractor/stocktwits_AAPL_withsentiment.csv', parse_dates=['date'], infer_datetime_format=True)\n",
    "            dataAMZN = pd.read_csv('data-extractor/stocktwits_AMZN_withsentiment.csv', parse_dates=['date'], infer_datetime_format=True)\n",
    "            dataGOOGL = pd.read_csv('data-extractor/stocktwits_GOOGL_withsentiment.csv', parse_dates=['date'], infer_datetime_format=True)\n",
    "            dataAAPL = dataAAPL.groupby(['date','sentiment'], sort=False).count()\n",
    "            dataAMZN = dataAMZN.groupby(['date','sentiment'], sort=False).count()\n",
    "            dataGOOGL = dataGOOGL.groupby(['date','sentiment'], sort=False).count()\n",
    "            dataAAPL = LoadData.combine_price_and_sentiment(dataAAPL, priceAAPL)\n",
    "            dataAMZN = LoadData.combine_price_and_sentiment(dataAMZN, priceAMZN)\n",
    "            dataGOOGL = LoadData.combine_price_and_sentiment(dataGOOGL, priceGOOGL)\n",
    "            dataAAPL.to_csv('data-extractor/stocktwits_AAPL_sharedata.csv', index=False)\n",
    "            dataAMZN.to_csv('data-extractor/stocktwits_AMZN_sharedata.csv', index=False)\n",
    "            dataGOOGL.to_csv('data-extractor/stocktwits_GOOGL_sharedata.csv', index=False)\n",
    "\n",
    "        dataAAPL = pd.read_csv('data-extractor/stocktwits_AAPL_sharedata.csv', parse_dates=['date'], infer_datetime_format=True)\n",
    "        dataAMZN = pd.read_csv('data-extractor/stocktwits_AMZN_sharedata.csv', parse_dates=['date'], infer_datetime_format=True)\n",
    "        dataGOOGL = pd.read_csv('data-extractor/stocktwits_GOOGL_sharedata.csv', parse_dates=['date'], infer_datetime_format=True)\n",
    "\n",
    "        return dataAAPL, dataAMZN, dataGOOGL\n",
    "\n",
    "    @classmethod\n",
    "    def get_stock_prediction_data(cls, symbol='ALL', type='training'):\n",
    "\n",
    "        \"\"\"\n",
    "            get the training and test data for stock prediction in format\n",
    "            (sentiment_calculated_bullish, sentiment_calculated_bearish, sentiment_actual_previous,\n",
    "            tweet_volume_change, cash_volume, label)\n",
    "            Standardize the data before using.\n",
    "        \"\"\"\n",
    "\n",
    "        file_location = 'data-extractor/stockdata_'+symbol+'_'+type+'.csv'\n",
    "        if not os.path.isfile(file_location):\n",
    "            import numpy as np\n",
    "\n",
    "            dataAAPL, dataAMZN, dataGOOGL = LoadData.aggregate_stock_price_data()\n",
    "            combined_data = dataAAPL.append([dataAMZN, dataGOOGL], ignore_index=True)\n",
    "            combined_data.sort_values('date')\n",
    "            combined_data.drop(columns='date', inplace=True)\n",
    "            combined_training, combined_test = np.split(combined_data.sample(frac=1), [int(.9*len(combined_data))])\n",
    "            combined_training.to_csv('data-extractor/stockdata_ALL_training.csv', index=False)\n",
    "            combined_test.to_csv('data-extractor/stockdata_ALL_test.csv', index=False)\n",
    "\n",
    "            dataAAPL.sort_values('date')\n",
    "            dataAAPL.drop(columns='date', inplace=True)\n",
    "            AAPL_training, AAPL_test = np.split(dataAAPL.sample(frac=1), [int(.9*len(dataAAPL))])\n",
    "            AAPL_training.to_csv('data-extractor/stockdata_AAPL_training.csv', index=False)\n",
    "            AAPL_test.to_csv('data-extractor/stockdata_AAPL_test.csv', index=False)\n",
    "\n",
    "            dataAMZN.sort_values('date')\n",
    "            dataAMZN.drop(columns='date', inplace=True)\n",
    "            AMZN_training, AMZN_test = np.split(dataAMZN.sample(frac=1), [int(.9*len(dataAMZN))])\n",
    "            AMZN_training.to_csv('data-extractor/stockdata_AMZN_training.csv', index=False)\n",
    "            AMZN_test.to_csv('data-extractor/stockdata_AMZN_test.csv', index=False)\n",
    "\n",
    "            dataGOOGL.sort_values('date')\n",
    "            dataGOOGL.drop(columns='date', inplace=True)\n",
    "            GOOGL_training, GOOGL_test = np.split(dataGOOGL.sample(frac=1), [int(.9*len(dataGOOGL))])\n",
    "            GOOGL_training.to_csv('data-extractor/stockdata_GOOGL_training.csv', index=False)\n",
    "            GOOGL_test.to_csv('data-extractor/stockdata_GOOGL_test.csv', index=False)\n",
    "\n",
    "        data = pd.read_csv(file_location)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'load_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-095f97172bc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'load_data'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    perform sentiment analysis of stocktwits data\n",
    "\"\"\"\n",
    "\n",
    "import load_data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "class SentimentAnalysis:\n",
    "\n",
    "    @classmethod\n",
    "    def get_sentiword_score(cls, message):\n",
    "        \"\"\"\n",
    "            takes a message and performs following operations:\n",
    "            1) tokenize\n",
    "            2) POS tagging\n",
    "            3) reduce text to nouns, verbs, adjectives, adverbs\n",
    "            4) lemmatize the words\n",
    "            for each selected tag, if more than one sense exists, performs word sense disambiguation\n",
    "            using lesk algorithm and finally returns positivity score, negativity score from\n",
    "            sentiwordnet lexicon\n",
    "        \"\"\"\n",
    "\n",
    "        tokens = word_tokenize(message)\n",
    "        pos = pos_tag(tokens)\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        selected_tags = list()\n",
    "        scores = list()\n",
    "\n",
    "        for i in range(len(pos)):\n",
    "            if pos[i][1].startswith('J'):\n",
    "                selected_tags.append((lemmatizer.lemmatize(pos[i][0], 'a'), 'a'))\n",
    "            elif pos[i][1].startswith('V'):\n",
    "                selected_tags.append((lemmatizer.lemmatize(pos[i][0], 'v'), 'v'))\n",
    "            elif pos[i][1].startswith('N'):\n",
    "                selected_tags.append((lemmatizer.lemmatize(pos[i][0], 'n'), 'n'))\n",
    "            elif pos[i][1].startswith('R'):\n",
    "                selected_tags.append((lemmatizer.lemmatize(pos[i][0], 'r'), 'r'))\n",
    "\n",
    "        # score list: [(sense name, pos score, neg score)]\n",
    "        for i in range(len(selected_tags)):\n",
    "            senses = list(swn.senti_synsets(selected_tags[i][0], selected_tags[i][1]))\n",
    "            if len(senses) == 1:\n",
    "                scores.append((senses[0].synset.name(), senses[0].pos_score(), senses[0].neg_score()))\n",
    "            elif len(senses) > 1:\n",
    "                sense = lesk(tokens, selected_tags[i][0], selected_tags[i][1])\n",
    "                if sense is None:\n",
    "                    # take average score of all original senses\n",
    "                    pos_score = 0\n",
    "                    neg_score = 0\n",
    "                    for i in senses:\n",
    "                        pos_score += i.pos_score()\n",
    "                        neg_score += i.neg_score()\n",
    "                    scores.append((senses[0].synset.name(), pos_score/len(senses), neg_score/len(senses)))\n",
    "                else:\n",
    "                    sense = swn.senti_synset(sense.name())\n",
    "                    scores.append((sense.synset.name(), sense.pos_score(), sense.neg_score()))\n",
    "\n",
    "        \"\"\"\n",
    "            there are a number of ways for aggregating sentiment scores\n",
    "            1) sum up all scores\n",
    "            2) average all scores (or only for non zero scores)\n",
    "            3) (1) or (2) but only for adjectives\n",
    "            4) if pos score greater than neg score +1 vote else -1 vote\n",
    "            here we are summing up the positive and negative scores to be used by classifier.\n",
    "            whenever we encounter a negative word, we reverse the positive and negative score.\n",
    "        \"\"\"\n",
    "\n",
    "        # collected from word stat financial dictionary\n",
    "        negation_words = list(open('data-extractor/lexicon_negation_words.txt').read().split())\n",
    "\n",
    "        # final_score = 0\n",
    "        # counter = 1\n",
    "        # for score in scores:\n",
    "        #     if any(score[0].startswith(x) for x in negation_words):\n",
    "        #         counter *= -1\n",
    "        #     else:\n",
    "        #         if score[1] > score[2]:\n",
    "        #             final_score += counter*score[1]\n",
    "        #         elif score[1] < score[2]:\n",
    "        #             final_score -= counter*score[2]\n",
    "\n",
    "        counter = 1\n",
    "        pos_score = 0\n",
    "        neg_score = 0\n",
    "        for score in scores:\n",
    "            if any(score[0].startswith(x) for x in negation_words):\n",
    "                counter *= -1\n",
    "            else:\n",
    "                if counter == 1:\n",
    "                    pos_score += score[1]\n",
    "                    neg_score += score[2]\n",
    "                elif counter == -1:\n",
    "                    pos_score += score[2]\n",
    "                    neg_score += score[1]\n",
    "\n",
    "        final_score = [pos_score, neg_score]\n",
    "        print(final_score)\n",
    "        return final_score\n",
    "\n",
    "    @classmethod\n",
    "    def sentiword_data_analysis(cls, symbol):\n",
    "        file_location = 'data-extractor/stocktwits_'+symbol+'_sentiwordnet_scored.csv'\n",
    "        if os.path.isfile(file_location) is False:\n",
    "            dataFrame = load_data.LoadData.get_stocktwits_data(symbol)\n",
    "            dataFrame['sentiwordnet_score'] = dataFrame.apply(lambda x: SentimentAnalysis.get_sentiword_score(x['message']), axis = 1)\n",
    "            dataFrame.to_csv(file_location, index=False)\n",
    "\n",
    "        dataFrame = pd.read_csv(file_location)\n",
    "        plt.hist(dataFrame['sentiwordnet_score'], bins=np.arange(-3.5, 4, 0.1), label=symbol)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()\n",
    "\n",
    "    @classmethod\n",
    "    def labelled_data_sentiwordnet_analysis(cls):\n",
    "        file_location = 'data-extractor/labelled_data_sentiwordnet_scored.csv'\n",
    "        if os.path.isfile(file_location) is False:\n",
    "            dataFrame = load_data.LoadData.get_labelled_data()\n",
    "            dataFrame['sentiwordnet_score'] = dataFrame.apply(lambda x: SentimentAnalysis.get_sentiword_score(x['message']), axis = 1)\n",
    "            dataFrame.to_csv(file_location, index=False)\n",
    "\n",
    "        dataFrame = pd.read_csv(file_location)\n",
    "        plt.hist(dataFrame[dataFrame['sentiment']=='Bullish']['sentiwordnet_score'], bins=np.arange(-3.5, 4, 0.1), label='Bullish', alpha=0.5)\n",
    "        plt.hist(dataFrame[dataFrame['sentiment']=='Bearish']['sentiwordnet_score'], bins=np.arange(-3.5, 4, 0.1), label='Bearish', alpha=0.5)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
